# ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models

## TL;DR 内容简介

- A corpus of 4k argumentative essays written by 7 GPT models. 【我们用7个GPT模型生成了4k议论文。】
- Linguistic analyses comparing machine-authored and human-authored essays. 【分析了机器生成与人类写作议论文的语言学特征，并进行了对比。】
- Several classifiers for detecting GPT-generated argumentative essays (99% accuracy at doc-level; 93% at sentence-level). 【训练了相关模型检测议论文是否由GPT模型生成（文章检测达到99%的准确率，句子检测达到93%的准确率）。】


## Related resources 相关资源

We are going to make related resources public, including our corpus, detection models, and an app that can be easily used by those English teachers who don't code. We are still building on these resources. They will be released soon.

我们近期会发布本研究用到的相关资源，包括语料库、检测模型、应用（不需要写代码，没有编程背景的英语老师同样可以使用）。很多资源还在建设中，请大家保持关注！

- ArguGPT corpus: We will release machine-generated argumentative essays soon. However, we cannot release human essays since we don't have copyright. WECCL can be found in (Wen, 2008); TOEFL11 can be purchased at https://catalog.ldc.upenn.edu/LDC2014T06; GRE is collected from GRE-prep materials. 【机器生成文章即将发布。我们没有权限发布人类文章。WECCL由文秋芳老师发布于《中国学生英语口笔语语料库》（SWECCL 2.0）；TOEFL11的可以在Linguistic Data Constortium上购买；GRE文章从范文书中收集，我们同样无权限发布。】
- Models: We fine-tuned RoBERTa to detect machine-generated argumentative essays. We trained both [doc-level](https://huggingface.co/SJTU-CL/RoBERTa-large-ArguGPT) and [sentence-level](https://huggingface.co/SJTU-CL/RoBERTa-large-ArguGPT-sent) classification. 【我们微调了RoBERTa以检测机器生成的议论文，我们在[文档级别](https://huggingface.co/SJTU-CL/RoBERTa-large-ArguGPT)和[句子级别](https://huggingface.co/SJTU-CL/RoBERTa-large-ArguGPT-sent)上都进行了训练。】
- App-demo: We build [an app](https://huggingface.co/spaces/SJTU-CL/argugpt-detector) in huggingface space which bases on sentence-level detector. 【基于句子级别的检测模型，我们搭建了一个[App](https://huggingface.co/spaces/SJTU-CL/argugpt-detector)，可以在网页中直接使用。】
- Paper: We release our [research paper](https://arxiv.org/abs/2304.07666) in arxiv. 【我们的[论文](https://arxiv.org/abs/2304.07666)预印在了arxiv中。】


## Data split and baseline 数据划分及基准模型

We first split the data into train/dev/test sets. The test split of TOEFL essays are as well evaluated by human participants in the Turing test. We established baselines for detecting our ArguGPT dataset by training detectors on SVMs and RoBERTa. Moreover, we conducted ablation study to see the effect of reducing training data points. 

我们将数据集进行了划分。其中，图灵测试（人类测评）中使用的数据为测试集中的托福作文。在此数据集上，我们训练了SVM和RoBERTa模型，作为该数据集的基准模型。此外，我们在不同大小的训练集上进行了训练。


| split | TOEFL | WECCL | GRE | total |
|-------|-------|-------|-----|-------|
| train | 3,058 | 2,715 | 980 | 6,753 |
| dev   | 300   | 300   | 100 | 700   |
| test  | 300   | 300   | 100 | 700   |


Accuracy of human evaluators on the TOEFL split of test set is only `64.65%`, far lagging behind ML detectors/classifiers. 【对测试集中的托福文章进行图灵测试，人类参与者的准确率仅有`64.65%`，远低于基于机器学习的检测器。】


| train data | test data | maj. bsln | RoBERTa | Best SVM |
|------------|-----------|-----------|---------|----------|
| doc-all    | doc test  | 50        | 99.38   | 95.14    |
| doc-50%    | doc test  | 50        | 99.76   | 94.14    |
| doc-25%    | doc test  | 50        | 99.14   | 93.86    |
| doc-10%    | doc test  | 50        | 97.67   | 92.29    |
| doc-all    | sent test | 54.18     | 49.73   | 72.15    |
| sent-all   | sent test | 54.18     | 93.84   | 81       |



## Team members 团队介绍

We are a group of students who are interested in language, linguistics, and NLP as well. Led by [Hai Hu](https://huhailinguist.github.io/), an assistant professor from English Department of Shanghai Jiao Tong University (SJTU), whose research interest is  computational linguistics, we hope to contribute something interesting to CL and NLP commuities from the perspective of language leaners. 

大家好，我们是一群热爱语言、同时也对NLP技术感兴趣的本科/研究生，团队由交大外院的[胡海](https://huhailinguist.github.io/)老师指导。我们会从语言学习者的角度，进行一些有趣的实验与研究。如果有幸的话，希望我们的研究能够对计算语言学和NLP社群做出一些小小的贡献！


| Name          | 姓名           | Affiliation | 所属机构       | Status              |
|---------------|---------------|-------------|---------------|---------------------|
| Hai Hu        | 胡海           | SFL, SJTU   | 上海交通大学外院 | Assistant Professor |
| Yiwen Zhang   | 张伊文         | Amazon      | 亚马逊         | Language Engineer   |
| Shisen Yue    | 岳士森         | SFL, SJTU   | 上海交通大学外院 | Undergraduate       |
| Wanyang Zhang | 章万扬         | SFL, HUST   | 华中科技大学外院 | Undergraduate       |
| Xiaojing Zhao | 赵晓婧         | SFL, SJTU   | 上海交通大学外院 | Graduate            |
| Xinyuan Cheng | 程心远         | SFL, SJTU   | 上海交通大学外院 | Undergraduate       |
| Yikang Liu    | 刘逸康         | SFL, HUST   | 华中科技大学外院 | Undergraduate       |
| Ziyin Zhang   | 张子殷         | SEIEE, SJTU | 上海交通大学电院 | Undergraduate       |


## Citation

Please cite our work as  

```
@misc{liu2023argugpt,
      title={ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models}, 
      author={Yikang Liu and Ziyin Zhang and Wanyang Zhang and Shisen Yue and Xiaojing Zhao and Xinyuan Cheng and Yiwen Zhang and Hai Hu},
      year={2023},
      eprint={2304.07666},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

```
