# ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models

## TL;DR

- A corpus of 4k argumentative essays written by 7 GPT models. 
- Linguistic analyses comparing machine-authored and human-authored essays. 
- Several classifiers for detecting GPT-generated argumentative essays (99% accuracy at doc-level; 93% at sentence-level).


## 内容速看

- 我们用7个GPT模型生成了4k议论文。
- 分析了机器生成与人类写作议论文的语言学特征，并进行了对比。
- 训练了相关模型检测议论文是否由GPT模型生成（文章检测达到99%的准确率，句子检测达到93%的准确率）。


## Related resources

We are going to make related resources public, including our corpus, detection models, and an app that can be easily used by those English teachers who don't code. We are still building on these resources. They will be released soon.

- ArguGPT corpus: We will release machine-generated argumentative essays soon. However, we cannot release human essays since we don't have copyright. WECCL can be found in (Wen, 2008); TOEFL11 can be purchased at https://catalog.ldc.upenn.edu/LDC2014T06; GRE is collected from GRE-prep materials. 
- Models: We fine-tuned RoBERTa to detect machine-generated argumentative essays. We trained both [doc-level](https://huggingface.co/SJTU-CL/RoBERTa-large-ArguGPT) and [sentence-level](https://huggingface.co/SJTU-CL/RoBERTa-large-ArguGPT-sent) classification.
- App-demo: We build [an app](https://huggingface.co/spaces/SJTU-CL/argugpt-detector) in huggingface space which bases on sentence-level detector.
- Paper: We release our [research paper](https://arxiv.org/abs/2304.07666) in arxiv.

## 相关资源

我们将发布本研究用到的相关资源，包括语料库、检测模型、应用（不需要写代码，没有编程背景的英语老师同样可以使用）。很多资源还在建设中，请大家保持关注！

- ArguGPT语料库：机器生成文章即将发布。我们没有权限发布人类文章。WECCL由文秋芳老师发布于《中国学生英语口笔语语料库》（SWECCL 2.0）；TOEFL11的可以在Linguistic Data Constortium上购买；GRE文章从范文书中收集，我们同样无权限发布。
- 模型：我们微调了RoBERTa以检测机器生成的议论文，我们在[文档级别](https://huggingface.co/SJTU-CL/RoBERTa-large-ArguGPT)和[句子级别](https://huggingface.co/SJTU-CL/RoBERTa-large-ArguGPT-sent)上都进行了训练。
- 网页应用：基于句子级别的检测模型，我们搭建了一个[App](https://huggingface.co/spaces/SJTU-CL/argugpt-detector)，可以在网页中直接使用。
- 论文：我们的[论文](https://arxiv.org/abs/2304.07666)预印在了arxiv中。

## Introduction

Content generated by artificial intelligence models have presented considerable challenge to educators around the world. When students submit AI generated content
(AIGC) as their own work, instructors will need to be able to detect such text, either
with the naked eye or with the help of some tools. There is also growing need to
understand the lexical, syntactic and stylistic features of AIGC.

To address these challenges in the context of English language teaching, we first
present ArguGPT, a carefully balanced corpus of 4,038 argumentative essays generated
by 7 GPT models in response to essay prompts from three sources: (1) in-class or
homework exercises, (2) TOEFL writing tasks and (3) GRE writing tasks. These
machine-generated texts are paired with roughly equal number of human-written essays
with low, medium and high scores matched in essay prompts.

We then hire English instructors to distinguish machine essays from human ones.
Results show that when first exposed to machine-generated essays, the instructors only
have an accuracy of 61 percent in detecting them. But the number rises to 67 percent
after one round of minimal self-training. Next, we perform linguistic analyses of the
machine and human essays, which show that machines produce sentences with more
complex syntactic structures while human essays tend to be lexically more complex.
Finally, we test existing AIGC detectors and build our own detectors using SVMs as
well as the RoBERTa model. Our results suggest that a RoBERTa fine-tuned with
the training set of ArguGPT can achieve above 90% accuracy in both essay-level and
sentence-level classification.

To the best of our knowledge, this is the first comprehensive analysis of argumentative essays produced by generative large language models. Our work demonstrates
the need for educators to acquaint themselves with AIGC, presents the characteristics of AI generated argumentative essays and shows that detecting AIGC from
the same domain seems to be an easy task for machine-learning based classifiers.





## 项目简介

AI生成内容（AIGC）对全世界的教育工作者带来了巨大挑战。当学生将AIGC作为作业提交时，不论是肉眼识别还是借助某些工具，教育工作者都有必要识别AI生成的内容，并了解AIGC的词汇、句法和文体特征。
为了解决该问题，首先，我们建立了ArguGPT语料库。ArguGPT由7个GPT（从GPT2到ChatGPT）模型生成的4,038篇英文议论文，共632个作文题目，包括：中国大学生课内或课后写作（25个题目）、托福独立写作（8个题目）和GRE issue写作（590个题目）。ArguGPT还收集了相当数量且写作水平分布较为平均的4115篇人类文章，绝大部分为二语学习者作文。
其次，我们聘招募了43名英语教师/硕博士生区分机器作文和人类作文。结果显示，初次接触到机器生成的文章时，识别准确率只有61%。但在一轮的简单的“培训”后，准确率上升至67%。与此同时，教师对学生作文识别的准确率为77%，对机器作文的识别率仅为51%（约等于随机猜），说明教师更加熟悉学生作文的特征。一个有趣的发现是，学生作文水平越差，越容易被识别，而机器作文水平越高，越容易被识别。
接下来，我们对机器和人类作文进行了语言学分析。初步结果表明，机器生成的句子，其句法结构更复杂；而人类则会使用更多样化的词汇。同时，七个模型生成文章的词汇复杂度和句法复杂度伴随模型能力的提升，基本呈线性增长。
最后，我们测试了一些现有的AIGC检测器，发现GPTzero的准确率超过90%。同时我们自行训练了SVM分类器以及RoBERTa分类器。在仅仅使用功能词或者POS Unigram时，SVM分类器准确率可以达到90%以上。我们使用RoBERTa-large微调的模型准确率最高，在文本级别能达到99%，在句子级别能达到93%以上。
我们的工作表明，英语教育工作者有必要进一步了解AIGC。同时，在英语作文领域，AIGC检测器能达到较高准确率。


## Team members

We are a group of students who are interested in language, linguistics, and NLP as well. Led by [Hai Hu](https://huhailinguist.github.io/), an assistant professor from English Department of Shanghai Jiao Tong University (SJTU), whose research interest is  computational linguistics, we hope to contribute something interesting to CL and NLP commuities from the perspective of language leaners. 

## 团队成员

大家好，我们是一群热爱语言、同时也对NLP技术感兴趣的本科/研究生，团队由交大外院的[胡海](https://huhailinguist.github.io/)老师指导。我们会从语言学习者的角度，进行一些有趣的实验与研究。如果有幸的话，希望我们的研究能够对计算语言学和NLP社群做出一些小小的贡献！


| Name          | 姓名           | Affiliation | 所属机构       | Status              |
|---------------|---------------|-------------|---------------|---------------------|
| Hai Hu        | 胡海           | SFL, SJTU   | 上海交通大学外院 | Assistant Professor |
| Yiwen Zhang   | 张伊文         | Amazon      | 亚马逊         | Language Engineer   |
| Shisen Yue    | 岳士森         | SFL, SJTU   | 上海交通大学外院 | Undergraduate       |
| Wanyang Zhang | 章万扬         | SFL, HUST   | 华中科技大学外院 | Undergraduate       |
| Xiaojing Zhao | 赵晓婧         | SFL, SJTU   | 上海交通大学外院 | Graduate            |
| Xinyuan Cheng | 程心远         | SFL, SJTU   | 上海交通大学外院 | Undergraduate       |
| Yikang Liu    | 刘逸康         | SFL, HUST   | 华中科技大学外院 | Undergraduate       |
| Ziyin Zhang   | 张子殷         | SEIEE, SJTU | 上海交通大学电院 | Undergraduate       |


## Citation

Please cite our work as  

```
@misc{liu2023argugpt,
      title={ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models}, 
      author={Yikang Liu and Ziyin Zhang and Wanyang Zhang and Shisen Yue and Xiaojing Zhao and Xinyuan Cheng and Yiwen Zhang and Hai Hu},
      year={2023},
      eprint={2304.07666},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

```
